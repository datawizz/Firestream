#!/usr/bin/env python3
"""
{{ app_name }}

{{ description }}
"""

import sys
import logging
from typing import Optional, Dict, Any, List
from datetime import datetime

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.types import *
{% if delta_enabled %}from delta import *{% endif %}

{% if config_enabled %}
from .config import AppConfig, load_config
{% else %}
import argparse
{% endif %}
{% if include_utils %}
from .utils import *
{% endif %}

# Configure logging
logging.basicConfig(
    level=logging.{% if log_level %}{{ log_level }}{% else %}INFO{% endif %},
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class {% if main_class_name %}{{ main_class_name }}{% else %}SparkApp{% endif %}:
    """Main Spark application class."""
    
    def __init__(self{% if config_enabled %}, config: AppConfig{% endif %}):
        """Initialize the Spark application."""
        {% if config_enabled %}
        self.config = config
        {% endif %}
        self.spark = self._create_spark_session()
        
    def _create_spark_session(self) -> SparkSession:
        """Create and configure Spark session."""
        {% if config_enabled %}
        logger.info(f"Creating Spark session for {self.config.app_name}")
        {% else %}
        logger.info("Creating Spark session")
        {% endif %}
        
        builder = SparkSession.builder \
            .appName({% if config_enabled %}self.config.app_name{% else %}"{{ app_name }}"{% endif %})
        
        # Add common configurations
        builder.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        
        {% if s3_enabled %}
        # Add S3 configuration if needed
        {% if config_enabled %}
        if self.config.input_path.startswith("s3") or self.config.output_path.startswith("s3"):
        {% endif %}
            builder.config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
                .config("spark.hadoop.fs.s3a.fast.upload", "true")
        {% endif %}
        
        {% if delta_enabled %}
        # Configure Delta Lake
        builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        {% endif %}
        
        {% for conf in spark_configs %}
        builder.config("{{ conf.key }}", "{{ conf.value }}")
        {% endfor %}
        
        return builder.getOrCreate()
    
    def process_data(self) -> DataFrame:
        """Main data processing logic."""
        {% if config_enabled %}
        logger.info(f"Reading data from {self.config.input_path}")
        input_path = self.config.input_path
        {% else %}
        input_path = sys.argv[1] if len(sys.argv) > 1 else "{% if default_input_path %}{{ default_input_path }}{% else %}input{% endif %}"
        logger.info(f"Reading data from {input_path}")
        {% endif %}
        
        # Read input data
        {% if input_format == "json" %}
        df = self.spark.read.json(input_path)
        {% elif input_format == "parquet" %}
        df = self.spark.read.parquet(input_path)
        {% elif input_format == "csv" %}
        df = self.spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(input_path)
        {% elif input_format == "delta" %}
        df = self.spark.read.format("delta").load(input_path)
        {% else %}
        df = self.spark.read.load(input_path)
        {% endif %}
        
        {% if cache_input %}
        # Cache the input data
        df.cache()
        {% endif %}
        
        logger.info(f"Input data count: {df.count()}")
        logger.info(f"Schema: {df.schema}")
        
        # Perform transformations
        {% for transform in transformations %}
        df = df.{{ transform.method }}({{ transform.args | join(sep=", ") }})
        {% endfor %}
        
        {% if custom_transformation_code %}
        # Custom transformations
        {{ custom_transformation_code }}
        {% endif %}
        
        {% if aggregations %}
        # Perform aggregations
        df = df.groupBy({% for col in group_by_columns %}"{{ col }}"{% if not loop.last %}, {% endif %}{% endfor %}) \
            .agg(
                {% for agg in aggregations %}
                F.{{ agg.function }}("{{ agg.column }}").alias("{{ agg.alias }}"){% if not loop.last %},{% endif %}
                {% endfor %}
            )
        {% endif %}
        
        {% if cache_input %}
        # Unpersist cached data
        df.unpersist()
        {% endif %}
        
        return df
    
    def write_output(self, df: DataFrame) -> None:
        """Write output data."""
        {% if config_enabled %}
        output_path = self.config.output_path
        {% else %}
        output_path = sys.argv[2] if len(sys.argv) > 2 else "{% if default_output_path %}{{ default_output_path }}{% else %}output{% endif %}"
        {% endif %}
        
        logger.info(f"Writing output to {output_path}")
        
        writer = df{% if config_enabled %}{% if config_fields %}.repartition(self.config.num_partitions){% endif %}{% endif %} \
            .write \
            .mode("{% if output_mode %}{{ output_mode }}{% else %}overwrite{% endif %}")
        
        {% if output_format == "json" %}
        writer.json(output_path)
        {% elif output_format == "parquet" %}
        writer.option("compression", "{% if compression %}{{ compression }}{% else %}snappy{% endif %}") \
            .parquet(output_path)
        {% elif output_format == "csv" %}
        writer.option("header", "true") \
            .csv(output_path)
        {% elif output_format == "delta" %}
        writer.format("delta") \
            .save(output_path)
        {% else %}
        writer.save(output_path)
        {% endif %}
        
        logger.info("Output written successfully")
    
    def run(self) -> None:
        """Run the Spark application."""
        try:
            {% if checkpoint_enabled and config_enabled %}
            # Set checkpoint directory if provided
            if hasattr(self.config, 'checkpoint_dir') and self.config.checkpoint_dir:
                self.spark.sparkContext.setCheckpointDir(self.config.checkpoint_dir)
            {% endif %}
            
            # Process data
            result_df = self.process_data()
            
            # Write output
            self.write_output(result_df)
            
            logger.info("Application completed successfully!")
            
        except Exception as e:
            logger.error(f"Application failed: {str(e)}")
            raise
        finally:
            self.spark.stop()


def main():
    """Main entry point."""
    {% if config_enabled %}
    # Load configuration
    config = load_config(sys.argv[1:])
    
    # Create and run application
    app = {% if main_class_name %}{{ main_class_name }}{% else %}SparkApp{% endif %}(config)
    {% else %}
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="{{ description }}")
    parser.add_argument("input_path", help="Input data path")
    parser.add_argument("output_path", help="Output data path")
    {% for arg in additional_arguments %}
    parser.add_argument("{{ arg.name }}", help="{{ arg.help }}"{% if arg.default %}, default="{{ arg.default }}"{% endif %})
    {% endfor %}
    args = parser.parse_args()
    
    # Create and run application
    app = {% if main_class_name %}{{ main_class_name }}{% else %}SparkApp{% endif %}()
    {% endif %}
    
    app.run()


if __name__ == "__main__":
    main()

{% if include_utils %}
# Additional utility functions
{% for util in utility_functions %}
{{ util }}
{% endfor %}
{% endif %}
