{
  "app_name": "Example PySpark App",
  "organization": "com.example",
  "version": "1.0.0",
  "description": "An example PySpark application for data processing",
  "package_name": "example_spark",
  "main_class_name": "ExampleApp",
  "author": "Example Author",
  "author_email": "author@example.com",
  "project_url": "https://github.com/example/pyspark-app",
  
  "python_version": "3.10",
  "python_major_version": "3",
  "pyspark_version": "3.5.1",
  
  "s3_enabled": true,
  "delta_enabled": false,
  "config_enabled": true,
  "checkpoint_enabled": false,
  "cache_input": true,
  "include_utils": true,
  "include_integration_test": true,
  "monitoring_enabled": true,
  "file_logging": true,
  "include_dev_dependencies": true,
  "use_pre_commit": true,
  
  "input_format": "parquet",
  "output_format": "parquet",
  "output_mode": "overwrite",
  "compression": "snappy",
  "config_format": "json",
  
  "config_fields": [
    {
      "name": "input_path",
      "python_type": "str",
      "default_value": "s3a://my-bucket/input",
      "env_var": "INPUT_PATH",
      "arg_index": 0,
      "description": "Input data path"
    },
    {
      "name": "output_path",
      "python_type": "str",
      "default_value": "s3a://my-bucket/output",
      "env_var": "OUTPUT_PATH",
      "arg_index": 1,
      "description": "Output data path"
    },
    {
      "name": "num_partitions",
      "python_type": "int",
      "python_default": "10",
      "default_value": "10",
      "env_var": "OUTPUT_PARTITIONS",
      "description": "Number of output partitions"
    }
  ],
  
  "transformations": [
    {
      "method": "filter",
      "args": ["F.col('value').isNotNull()"]
    },
    {
      "method": "withColumn",
      "args": ["'processed_date'", "F.current_date()"]
    },
    {
      "method": "withColumn",
      "args": ["'year'", "F.year(F.col('timestamp'))"]
    }
  ],
  
  "aggregations": [
    {
      "function": "count",
      "column": "*",
      "alias": "count"
    },
    {
      "function": "avg",
      "column": "value",
      "alias": "avg_value"
    },
    {
      "function": "max",
      "column": "value",
      "alias": "max_value"
    }
  ],
  
  "group_by_columns": ["category", "processed_date"],
  
  "spark_configs": [
    {
      "key": "spark.sql.adaptive.enabled",
      "value": "true"
    },
    {
      "key": "spark.sql.adaptive.coalescePartitions.enabled",
      "value": "true"
    }
  ],
  
  "docker_registry": "myregistry.io",
  "namespace": "spark-apps",
  "service_account": "spark",
  
  "driver_cores": 1,
  "driver_memory": "2g",
  "executor_cores": 2,
  "executor_instances": 3,
  "executor_memory": "4g",
  
  "spark_conf": {
    "spark.kubernetes.container.image.pullPolicy": "Always",
    "spark.hadoop.fs.s3a.access.key": "${S3_ACCESS_KEY}",
    "spark.hadoop.fs.s3a.secret.key": "${S3_SECRET_KEY}"
  },
  
  "driver_env_vars": [
    {
      "name": "S3_ACCESS_KEY",
      "valueFrom": {
        "secretKeyRef": {
          "name": "s3-credentials",
          "key": "access-key"
        }
      }
    },
    {
      "name": "S3_SECRET_KEY",
      "valueFrom": {
        "secretKeyRef": {
          "name": "s3-credentials",
          "key": "secret-key"
        }
      }
    }
  ],
  
  "volumes": [
    {
      "name": "spark-config",
      "configMap": {
        "name": "example-pyspark-app-config"
      }
    }
  ],
  
  "driver_volume_mounts": [
    {
      "name": "spark-config",
      "mountPath": "/opt/spark/conf"
    }
  ],
  
  "executor_volume_mounts": [
    {
      "name": "spark-config",
      "mountPath": "/opt/spark/conf"
    }
  ],
  
  "test_sample_data": [
    "('id1', 100.0, 'category1', datetime(2024, 1, 1))",
    "('id2', 200.0, 'category2', datetime(2024, 1, 2))",
    "('id3', 150.0, 'category1', datetime(2024, 1, 3))"
  ],
  "test_columns": ["id", "value", "category", "timestamp"],
  "test_schema": [
    ["id", "StringType"],
    ["value", "DoubleType"],
    ["category", "StringType"],
    ["timestamp", "TimestampType"]
  ],
  "expected_output_schema": [
    ["category", "StringType"],
    ["processed_date", "DateType"],
    ["count", "LongType"],
    ["avg_value", "DoubleType"],
    ["max_value", "DoubleType"]
  ],
  
  "log_level": "INFO",
  "log_file_path": "/tmp/pyspark-app.log",
  "log_file_size_bytes": 104857600,
  "include_log4j": true,
  "include_app_config": true,
  
  "file_logging": true,
  "log_file_size": "100MB",
  "log_file_count": 10,
  "custom_loggers": [],
  
  "max_line_length": 100,
  "local_run_args": ["local-input", "local-output"],
  
  "exported_utils": [
    "optimize_join",
    "write_with_retry",
    "add_metadata_columns",
    "calculate_statistics"
  ],
  
  "additional_pytest_markers": [
    {
      "name": "spark",
      "description": "marks tests that require Spark"
    }
  ],
  
  "license": "Apache-2.0",
  
  "custom_fixtures": [],
  
  "labels": {},
  "driver_labels": {},
  "executor_labels": {},
  "driver_node_selector": {},
  "executor_node_selector": {},
  "image_pull_secrets": [],
  "arguments": [],
  "driver_tolerations": [],
  "executor_tolerations": [],
  "hadoop_conf": {},
  "spark_event_log_enabled": false,
  "driver_core_limit": "1200m",
  "restart_policy_type": "OnFailure",
  "on_failure_retries": 3,
  "on_failure_retry_interval": 10,
  "on_submission_failure_retries": 5,
  "on_submission_failure_retry_interval": 20,
  "spark_ui_port": "4040",
  "dynamic_allocation": null,
  "py_files": [],
  "packages": [],
  "repositories": [],
  "image_pull_policy": "Always",
  "spark_event_log_dir": "/tmp/spark-events",
  "jmx_exporter_version": "0.11.0",
  "prometheus_port": 8090,
  
  "additional_pre_commit_hooks": [],
  "additional_tests": [],
  "utility_functions": [],
  "additional_arguments": [],
  "additional_config_files": [],
  "spark_uid": "185",
  "spark_base_image": "apache/spark-py:v3.5.1",
  "apt_packages": [],
  "config_files": [],
  "additional_files": [],
  "environment_vars": [],
  "custom_make_targets": [],
  "additional_gitignore_patterns": []
}
