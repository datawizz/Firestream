### Deployment ###
DEPLOYMENT_MODE='development'
COMPOSE_PROJECT_NAME=fireworks

### Github ###
GITHUB_SSH_KEY='' # Used for pulling private registries
GIT_COMMIT_HASH='development' # Filled in later using the actual commit hash. Used for image tags

### Docker ###

DOCKER_BUILDKIT=1
CONTAINER_REGISTRY_URL='localhost'
CONTAINER_REGISTRY_PORT=5000
DOCKER_GID=998 # TODO I dont think this is required?

### Host Configuration ##
# By using the host user id and group id, we can ensure that the files created by the container are owned by the host user
# Also enables rootless contianers
HOST_USER_ID=1000 # your user id, defaults to 1000 for debian hosts
HOST_GROUP_ID=1000 # your group id, defaults to 1000 for debian hosts
#HOST_OCI_ENGINE='docker' # your oci engine, defaults to docker. #TODO make this compatible with podman


### Framework Versions ###
# For packages which are built from source, we need to specify the version number and hash
SPARK_VERSION=3.3.2
SPARK_SHA512="347fd9029128b12e7b05e9cd7948a5b571a57f16bbbbffc8ad4023b4edc0e127cffd27d66fcdbf5f926fa33362a2ae4fc0a8d59ab3abdaa1d4c4ef1e23126932  spark-3.3.2-bin-without-hadoop.tgz"
SPARK_HOME=/opt/spark

HADOOP_VERSION=3.3.4
HADOOP_SHA512="ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4  hadoop-3.3.4.tar.gz"
HADOOP_HOME=/opt/hadoop

SOLR_VERSION=9.2.0
SOLR_SHA512="f10c5e15a52882501bc29af11855066238c798658a21c8ccf780dcbdd269a1534a2d06e47f743a2c6331d4e45ab9c976cf7099337422698c4418c6373d284665 *solr-9.2.0.tgz"

HIVE_VERSION=3.0.0
HIVE_SHA256="c8af45a92a60664f95811ca3011c34f50bf9614aab3e99f9fd10775c90b40847  hive-standalone-metastore-3.0.0-bin.tar.gz"


### Kafka ###

KAFKA_BOOTSTRAP_SERVER="kafka.default.svc.cluster.local"
KAFKA_BOOTSTRAP_SERVER_PORT="9092"

### S3 ###
# For a development deployment, we use a local minio instance
S3_ACCESS_KEY_ID='TODO_CHANGE_ME'
S3_SECRET_ACCESS_KEY='THIS_IS_A_SECRET_TODO_CHANGE_ME'
S3_DEFAULT_REGION='us-west-2'
S3_ENDPOINT_URL='http://minio.default.svc.cluster.local:9000'
S3_BUCKET_NAME='fireworks'

LOCAL_DATA_DIR='/workspace/tmp/data'

### MinIO Local Staging Semi-Permanent ###
# S3_ACCESS_KEY_ID='UTMs2xbPYleLRdKs1sT7'
# S3_SECRET_ACCESS_KEY='TUiPgi2obcId2ev164xk'
# S3_ENDPOINT_URL='http://10.0.0.232:9000'
# S3_DEFAULT_REGION='us-west-2'
# S3_BUCKET_NAME='fireworks-staging'

## Spark ##
# SPARK_LOCAL_IP="127.0.1.1"

### PostgreSQL ###
POSTGRES_USER='pguser'
POSTGRES_PASSWORD='THIS_IS_A_SECRET_TODO_CHANGE_ME'
POSTGRES_DEFAULT_DB='postgres'
POSTGRES_URL="postgresql.default.svc.cluster.local"
POSTGRES_PORT=5432
JDBC_CONNECTION_STRING=jdbc:postgresql://postgresql.default.svc.cluster.local:5432/postgres
DATABASE_URL=postgres://pguser:THIS_IS_A_SECRET_TODO_CHANGE_ME@postgresql.default.svc.cluster.local:5432/postgres

### Project Nessie ###
NESSIE_SERVER_URI="http://nessie.default.svc.cluster.local:19120/api/v1"
NESSIE_VERSION="0.58.1"

### Superset ###
SUPERSET_DATABASE_URI=postgresql://pguser:THIS_IS_A_SECRET_TODO_CHANGE_ME@postgresql:5432/postgres
SUPERSET_URL=http://superset.default.svc.cluster.local:8088
# TODO make a database in Superset from helm

### Hive Metastore ###
METASTORE_HOME=/opt/hive-metastore
METASTORE_DB_NAME=hive_metastore
METASTORE_URL="hive-metastore.default.svc.cluster.local"
METASTORE_PORT=9083

### Kyuubi ###
HIVE_THRIFT="kyuubi-thrift-binary.default.svc.cluster.local"

### HADOOP ###
HADOOP_HOME=/opt/hadoop

### Solr ###

SOLR_URL='http://solr.default.svc.cluster.local:8983'
SOLR_PORT=8983
SOLR_USERNAME='admin'
SOLR_PASSWORD='3gzOwNhko'
SOLR_DEFAULT_CORE='fireworks'

### Open Search ###
OPENSEARCH_URL='https://opensearch-cluster-master.default.svc.cluster.local:9200'
OPENSEARCH_USERNAME='admin'
OPENSEARCH_PASSWORD='admin'

### Supabase ###
# SITE_URL=http://192.0.2.0:3000
# API_EXTERNAL_URL=http://192.0.2.0:8000
# STUDIO_PORT=3000
# PUBLIC_REST_URL=http://192.0.2.0:8000/rest/v1/

# ### Customer Analytics ###
# REACT_APP_GOOGLE_ANALYTICS_GTAG='G-XXXXXXXXXX'
# REACT_APP_AMPLITUDE_ANALYTICS_API_KEY='XXXXXXXXX'
# REACT_APP_MIXPANEL_ANALYTICS_KEY='XXXXXXXXX'

# ### OPEN AI ###
# OPENAI_API_KEY='XXXXXXXXX'

# ### TODO parse this from Quadratic and condense
# REACT_APP_GOOGLE_ANALYTICS_GTAG=G-0000000000
# REACT_APP_AMPLITUDE_ANALYTICS_API_KEY=
# REACT_APP_MIXPANEL_ANALYTICS_KEY=
# REACT_APP_SENTRY_DSN=https://xxxxxxxxxxxxxxxxxx@xxxxxxxxxxxx.ingest.sentry.io/xxxxxxxxxxxx
# # // use =1 to enable debug flags
# REACT_APP_DEBUG=0 
# REACT_APP_AUTH0_DOMAIN=auth-dev.quadratic.to
# REACT_APP_AUTH0_CLIENT_ID=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# REACT_APP_AUTH0_AUDIENCE=https://localhost:8000
# REACT_APP_AUTH0_ISSUER=https://auth-dev.quadratic.to/
# REACT_APP_QUADRATIC_API_URL=http://localhost:8000
